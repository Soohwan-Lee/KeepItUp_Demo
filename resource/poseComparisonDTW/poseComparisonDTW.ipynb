{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DTW with recorded coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fastdtw\n",
      "  Downloading fastdtw-0.3.4.tar.gz (133 kB)\n",
      "     -------------------------------------- 133.4/133.4 kB 4.0 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: numpy in c:\\users\\leesoohwan\\anaconda3\\envs\\py38\\lib\\site-packages (from fastdtw) (1.23.5)\n",
      "Building wheels for collected packages: fastdtw\n",
      "  Building wheel for fastdtw (setup.py): started\n",
      "  Building wheel for fastdtw (setup.py): finished with status 'done'\n",
      "  Created wheel for fastdtw: filename=fastdtw-0.3.4-py3-none-any.whl size=3566 sha256=8dc73635b1d17f45f690722307b22a0dd5be93f785ce90962cdff3d5a28e75ba\n",
      "  Stored in directory: c:\\users\\leesoohwan\\appdata\\local\\pip\\cache\\wheels\\e9\\ac\\30\\c962f9d759dd68cb5482727c44441fdfb48040fdbe983857e8\n",
      "Successfully built fastdtw\n",
      "Installing collected packages: fastdtw\n",
      "Successfully installed fastdtw-0.3.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install fastdtw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import animation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import socket\n",
    "import time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-person Pose Estimation with MoveNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Optional if you are using a GPU\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "### Load Model\n",
    "model = hub.load('https://tfhub.dev/google/movenet/multipose/lightning/1')\n",
    "movenet = model.signatures['serving_default']\n",
    "\n",
    "### Load Video\n",
    "video_path = '../../data/videos/'\n",
    "sampleVideo = video_path + 'sampleVideo.mp4'\n",
    "basicFour = video_path + 'basic_four.mp4'\n",
    "basicTwo = video_path + 'basic_two.mp4'\n",
    "swingDiff = video_path + 'swing_diff.mp4'\n",
    "swingSame = video_path + 'swing_same.mp4'\n",
    "swingRaw = video_path + 'swingRaw.mp4'\n",
    "temporalDifficult = video_path + 'temporalDifficultRaw.mp4'\n",
    "temporalEasy = video_path + 'temporalEasyRaw.mp4'\n",
    "\n",
    "\n",
    "\n",
    "### Draw EDGES\n",
    "EDGES = {\n",
    "    (0, 1): 'm',\n",
    "    (0, 2): 'c',\n",
    "    (1, 3): 'm',\n",
    "    (2, 4): 'c',\n",
    "    (0, 5): 'm',\n",
    "    (0, 6): 'c',\n",
    "    (5, 7): 'm',\n",
    "    (7, 9): 'm',\n",
    "    (6, 8): 'c',\n",
    "    (8, 10): 'c',\n",
    "    (5, 6): 'y',\n",
    "    (5, 11): 'm',\n",
    "    (6, 12): 'c',\n",
    "    (11, 12): 'y',\n",
    "    (11, 13): 'm',\n",
    "    (13, 15): 'm',\n",
    "    (12, 14): 'c',\n",
    "    (14, 16): 'c'\n",
    "}\n",
    "\n",
    "### Vector List\n",
    "vectorList = [\n",
    "    [0,1],\n",
    "    [0,2],\n",
    "    [1,3],\n",
    "    [2,4],\n",
    "    [3,5],\n",
    "    [0,6],\n",
    "    [1,7],\n",
    "    [6,7],\n",
    "    [6,8],\n",
    "    [7,9],\n",
    "    [8,10],\n",
    "    [9,10]\n",
    "]\n",
    "\n",
    "# Function to loop through each person detected and render\n",
    "def loop_through_people(frame, keypoints_with_scores, edges, confidence_threshold):\n",
    "    for person in keypoints_with_scores:\n",
    "        draw_connections(frame, person, edges, confidence_threshold)\n",
    "        draw_keypoints(frame, person, confidence_threshold)\n",
    "\n",
    "\n",
    "def draw_keypoints(frame, keypoints, confidence_threshold):\n",
    "    y, x, c = frame.shape\n",
    "    shaped = np.squeeze(np.multiply(keypoints, [y,x,1]))\n",
    "    \n",
    "    for kp in shaped:\n",
    "        ky, kx, kp_conf = kp\n",
    "        if kp_conf > confidence_threshold:\n",
    "            cv2.circle(frame, (int(kx), int(ky)), 3, (0,255,0), -1)\n",
    "\n",
    "def draw_connections(frame, keypoints, edges, confidence_threshold):\n",
    "    y, x, c = frame.shape\n",
    "    shaped = np.squeeze(np.multiply(keypoints, [y,x,1]))\n",
    "    \n",
    "    for edge, color in edges.items():\n",
    "        p1, p2 = edge\n",
    "        y1, x1, c1 = shaped[p1]\n",
    "        y2, x2, c2 = shaped[p2]\n",
    "        \n",
    "        if (c1 > confidence_threshold) & (c2 > confidence_threshold):      \n",
    "            cv2.line(frame, (int(x1), int(y1)), (int(x2), int(y2)), (0,0,255), 2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare each coordinate with DTW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from scipy.spatial.distance import euclidean\n",
    "from fastdtw import fastdtw\n",
    "\n",
    "# assume video_path is the path to the video file\n",
    "# # video_path = \"path/to/video.mp4\"\n",
    "# video_path = '../../data/videos/'\n",
    "# sampleVideo = video_path + 'sampleVideo.mp4'\n",
    "# basicFour = video_path + 'basic_four.mp4'\n",
    "# basicTwo = video_path + 'basic_two.mp4'\n",
    "# swingDiff = video_path + 'swing_diff.mp4'\n",
    "# swingSame = video_path + 'swing_same.mp4'\n",
    "# swingRaw = video_path + 'swingRaw.mp4'\n",
    "# temporalDifficult = video_path + 'temporalDifficultRaw.mp4'\n",
    "# temporalEasy = video_path + 'temporalEasyRaw.mp4'\n",
    "\n",
    "# assume pose1_coords is a numpy array of shape (M, 2),\n",
    "# where M is the number of pose keypoints * 2\n",
    "pose1_coords = None\n",
    "\n",
    "# read the video using OpenCV\n",
    "cap = cv2.VideoCapture(swingSame)\n",
    "\n",
    "while cap.isOpened():\n",
    "    # read a frame from the video\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # # assume pose estimation code extracts the pose coordinates from the frame\n",
    "    # pose2 = pose_estimation(frame)\n",
    "\n",
    "    if pose1_coords is None:\n",
    "        # initialize the reference pose sequence with the first pose\n",
    "        pose1_coords = pose2.flatten()\n",
    "        continue\n",
    "\n",
    "    # compute the Euclidean distance between corresponding keypoints\n",
    "    distance = lambda a, b: euclidean(a, b)\n",
    "\n",
    "    # compute the DTW distance and path\n",
    "    distance, path = fastdtw(pose1_coords.reshape(1, -1), pose2.flatten().reshape(1, -1), dist=distance)\n",
    "\n",
    "    # compute the similarity between the poses (using a normalized distance)\n",
    "    similarity = 1.0 / (1.0 + distance)\n",
    "\n",
    "    # compute the time difference between the current frame and the reference frame\n",
    "    time_diff = (path[-1][0] - path[0][0]) / float(len(pose1_coords))\n",
    "\n",
    "    # update the reference pose sequence with the current pose\n",
    "    pose1_coords = pose2.flatten()\n",
    "\n",
    "    # print the results\n",
    "    print(\"Pose similarity: \", similarity)\n",
    "    print(\"Time difference: \", time_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
